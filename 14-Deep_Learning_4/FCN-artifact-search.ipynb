{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import VGG\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import random\n",
    "import sys\n",
    "if '/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path:\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "from torch.optim import lr_scheduler                                                                                                                                                                                       \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from zipfile import ZipFile\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FCN16s model for deconvolution layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1,\n",
    "                             padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "                             \n",
    "-  in_channels – the number of channels of the input signal\n",
    "-  out_channels – the number of channels generated by convolution\n",
    "-  kerner_size – size of convolution kernel\n",
    "-  stride – Convolution step size, that is, the multiple of the input to be expanded.\n",
    "-  padding – The height and width are increased by 2 * padding\n",
    "-  output_padding – the height and width are increased by padding\n",
    "-  groups – number of blocked connections from input channel to output channel\n",
    "-  bias – if bias = True, add bias\n",
    "-  dilation – spacing between elements of the convolution kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN16s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super(FCN16s, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace = True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        \n",
    "        # After the feature extraction layer of vgg, you can get the feature map. \n",
    "        # The size of the feature map after 5 max_pools are respectively\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "        x2 = output['x2']  # size=(N, 128, x.H/4,  x.W/4)\n",
    "        x1 = output['x1']  # size=(N, 64, x.H/2,  x.W/2)\n",
    "        \n",
    "#===========FCN16s model ==========================\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "#===========FCN16s model ==========================\n",
    "        \n",
    "#===========Please design a FCN8s model ===========\n",
    "        \n",
    "\n",
    "        \n",
    "#===========Please design a FCN8s model ===========\n",
    "\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define VGG16 for convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
    "        super(VGGNet, self).__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
    "            del self.classifier\n",
    "\n",
    "        if show_params:\n",
    "            for name, param in self.named_parameters():\n",
    "                print(name, param.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):      \n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
    "    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
    "    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
    "    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
    "}\n",
    "\n",
    "# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "epochs     = 25  #500\n",
    "lr         = 1e-4\n",
    "momentum   = 0\n",
    "w_decay    = 1e-5\n",
    "step_size  = 50\n",
    "gamma      = 0.5\n",
    "model_use  = \"subt_model\" # \"subt_model\"\n",
    "n_class = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://drive.google.com/u/1/uc?id=1ORB2iXh0Os-64WqIrE7rWJBsEvQsFTVs&export=download\"\n",
    "dataset_name = \"data\"\n",
    "if not os.path.isdir(dataset_name):\n",
    "    gdown.download(dataset_url, output=dataset_name + '.zip', quiet=False)\n",
    "    zip1 = ZipFile(dataset_name + '.zip')\n",
    "    zip1.extractall(dataset_name)\n",
    "    zip1.close()\n",
    "    os.remove(\"data.zip\")\n",
    "\n",
    "print(\"Finished downloading dataset.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define path, directory trainning environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "FullPath = os.getcwd()\n",
    "data_dir  = os.path.join(FullPath + \"/data/subt\")\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Data not found!\")\n",
    "    \n",
    "# create dir for model\n",
    "model_dir = os.path.join(FullPath + \"/models\", model_use)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# create dir for score\n",
    "score_dir = os.path.join(FullPath + \"/scores\", model_use)\n",
    "if not os.path.exists(score_dir):\n",
    "    os.makedirs(score_dir)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "num_gpu = list(range(torch.cuda.device_count()))\n",
    "\n",
    "vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n",
    "fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)\n",
    "\n",
    "if use_gpu:\n",
    "    ts = time.time()\n",
    "    vgg_model = vgg_model.cuda()\n",
    "    fcn_model = fcn_model.cuda()\n",
    "    fcn_model = nn.DataParallel(fcn_model, device_ids=num_gpu)\n",
    "    print(\"Finish cuda loading, time elapsed {}\".format(time.time() - ts))\n",
    "else:\n",
    "#     nn.DataParallel(fcn_model)\n",
    "    print(\"Use CPU to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fcn_model)\n",
    "params = list(fcn_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class\n",
    "-------------\n",
    "\n",
    "``torch.utils.data.Dataset`` is an abstract class representing a\n",
    "dataset.  \n",
    "Your custom dataset should inherit ``Dataset`` and override the following\n",
    "methods:\n",
    "\n",
    "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
    "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
    "   be used to get $i$\\ th sample\n",
    "\n",
    "Let's create a dataset class for our face landmarks dataset.  \n",
    "We will read the csv in ``__init__`` but leave the reading of images to\n",
    "``__getitem__``.   \n",
    "This is memory efficient because all the images are not\n",
    "stored in the memory at once but read as required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n",
    "h, w      = 480, 640\n",
    "val_h     = h\n",
    "val_w     = w\n",
    "\n",
    "class product_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, phase, n_class=n_class, flip_rate=0.):\n",
    "        data_dir = os.path.join(root, phase)\n",
    "        self.rgb_list = os.listdir(os.path.join(data_dir, 'images'))\n",
    "        _list = self.rgb_list\n",
    "        self.label_list = []\n",
    "        for i in range(len(self.rgb_list)):\n",
    "            self.label_list.append(_list[i].split(\".\")[0] + \".png\")\n",
    "\n",
    "        self.rgb_dir = os.path.join(data_dir, 'images')\n",
    "        self.label_dir = os.path.join(data_dir, 'masks')\n",
    "        self.means     = means\n",
    "        self.n_class   = n_class\n",
    "        self.flip_rate = flip_rate\n",
    "        if phase == 'train':\n",
    "            self.flip_rate = 0.5\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.rgb_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.rgb_list)        \n",
    "        img = cv2.imread(os.path.join(self.rgb_dir, self.rgb_list[idx]),cv2.IMREAD_UNCHANGED)\n",
    "        label = cv2.imread(os.path.join(self.label_dir, self.label_list[idx]), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #===SubT label error===backpack = 38, survivor = 75, vent = 113, phone = 14\n",
    "        label[label == 38] = 1\n",
    "        label[label == 75] = 2\n",
    "        label[label == 113] = 3\n",
    "        label[label == 14] = 4  \n",
    "        #===SubT error===       \n",
    "        \n",
    "        img = cv2.resize(img, (640, 480), interpolation=cv2.INTER_CUBIC)\n",
    "        label = cv2.resize(label, (640, 480), interpolation=cv2.INTER_CUBIC)\n",
    "   \n",
    "        origin_img = img\n",
    "        if random.random() < self.flip_rate:\n",
    "            img   = np.fliplr(img)\n",
    "            label = np.fliplr(label)\n",
    "\n",
    "        # reduce mean\n",
    "        img = img[:, :, ::-1]  # switch to BGR\n",
    "        \n",
    "        img = np.transpose(img, (2, 0, 1)) / 255.\n",
    "        img[0] -= self.means[0]\n",
    "        img[1] -= self.means[1]\n",
    "        img[2] -= self.means[2]\n",
    "\n",
    "        # convert to tensor\n",
    "        img = torch.from_numpy(img.copy()).float()\n",
    "        label = torch.from_numpy(label.copy()).long()\n",
    "\n",
    "        # create one-hot encoding\n",
    "        h, w = label.size()\n",
    "        target = torch.zeros(self.n_class, h, w)\n",
    "        \n",
    "        for i in range(n_class):\n",
    "            target[i][label == i] = 1\n",
    "        \n",
    "#         target[0][label == 0] = 1\n",
    "#         print(np.unique(label))\n",
    "        \n",
    " \n",
    "        sample = {'X': img, 'Y': target, 'l': label, 'origin': origin_img}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial dataloader for trainning and validation\n",
    "train_data = product_dataset(data_dir, phase = 'train')\n",
    "val_data   = product_dataset(data_dir, phase = 'test', flip_rate = 0)\n",
    "dataloader = DataLoader(train_data, batch_size = batch_size, shuffle=True, num_workers = 0)\n",
    "val_loader = DataLoader(val_data, batch_size = 1, num_workers = 0)\n",
    "\n",
    "dataiter = iter(val_loader)\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(fcn_model.parameters(), lr = lr, momentum = momentum, weight_decay = w_decay)\n",
    "# decay LR by a factor of 0.5 every step_size = 50 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        fcn_model.train()\n",
    "        scheduler.step()\n",
    "        configs    = \"FCNs_{}_batch{}_epoch{}_RMSprop_lr{}\"\\\n",
    "            .format(model_use, batch_size, epoch, lr)\n",
    "        model_path = os.path.join(model_dir, configs)\n",
    "        \n",
    "        ts = time.time()\n",
    "        for iter, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs = Variable(batch['X'].cuda())\n",
    "                labels = Variable(batch['Y'].cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(batch['X']), Variable(batch['Y'])\n",
    "\n",
    "            outputs = fcn_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 1 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch+1, iter, loss.item()))\n",
    "        \n",
    "        print(\"Finish epoch {}, time elapsed {}\".format(epoch+1, time.time() - ts))\n",
    "        if epoch % 1 == 0:\n",
    "            torch.save(fcn_model.state_dict(),model_path + '.pkl')\n",
    "\n",
    "        val(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    fcn_model.eval()\n",
    "    TP = np.zeros(n_class-1)\n",
    "    FN = np.zeros(n_class-1)\n",
    "    FP = np.zeros(n_class-1)\n",
    "    total_ious = []\n",
    "    pixel_accs = []\n",
    "    for iter, batch in enumerate(val_loader):\n",
    "        if use_gpu:\n",
    "            inputs = Variable(batch['X'].cuda())\n",
    "        else:\n",
    "            inputs = Variable(batch['X'])\n",
    "\n",
    "        output = fcn_model(inputs)\n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        N, _, h, w = output.shape\n",
    "        pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis=1).reshape(N, h, w)\n",
    "\n",
    "        target = batch['l'].cpu().numpy().reshape(N, h, w)\n",
    "        for p, t in zip(pred, target):\n",
    "            pixel_accs.append(pixel_acc(p, t))\n",
    "            _TP, _FN, _FP =  analysis(p, t, h, w)\n",
    "            TP += _TP[1:n_class]\n",
    "            FN += _FN[1:n_class]\n",
    "            FP += _FP[1:n_class]\n",
    "            \n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    ious = TP / (TP + FN + FP)\n",
    "    fscore = 2*TP / (2*TP + FN + FP)\n",
    "    total_ious = np.array(total_ious).T  # n_class * val_len\n",
    "    pixel_accs = np.array(pixel_accs).mean()\n",
    "    \n",
    "#     print(\"epoch{}, pix_acc: {}, meanIoU: {}, IoUs: {}, recall: {}, precision: {}, fscore: {}\"\\\n",
    "#           .format(epoch, pixel_accs, np.nanmean(ious), ious, recall, precision, fscore))\n",
    "    \n",
    "    f1 = open(score_dir + \"/cls_acc_log.txt\",\"a+\")\n",
    "    f1.write('epoch:'+ str(epoch) + ', pix_acc: ' + str(pixel_accs) + '\\n' )\n",
    "    f2 = open(score_dir + \"/cls_iou_log.txt\",\"a+\")\n",
    "    f2.write('epoch:'+ str(epoch) + ', class ious: ' + str(ious) + '\\n' )\n",
    "    f3 = open(score_dir + \"/mean_iou_log.txt\",\"a+\")\n",
    "    f3.write('epoch:'+ str(epoch) + ', mean IoU: ' + str(np.nanmean(ious)) + '\\n' ) \n",
    "    f4 = open(score_dir + \"/recall_log.txt\",\"a+\")\n",
    "    f4.write('epoch:'+ str(epoch) + ', class recall: ' + str(recall) + '\\n' )\n",
    "    f5 = open(score_dir + \"/precision_log.txt\",\"a+\")\n",
    "    f5.write('epoch:'+ str(epoch) + ', class precision: ' + str(precision) + '\\n' )    \n",
    "    f6 = open(score_dir + \"/fscore_log.txt\",\"a+\")\n",
    "    f6.write('epoch:'+ str(epoch) + ', class fscore: ' + str(fscore) + '\\n' )  \n",
    "    \n",
    "\n",
    "def analysis(pred, target, h, w):\n",
    "    # TP, FN, FP, TN\n",
    "    TP = np.zeros(n_class)\n",
    "    FN = np.zeros(n_class)\n",
    "    FP = np.zeros(n_class)\n",
    "\n",
    "    target = target.reshape(h * w)\n",
    "    pred = pred.reshape(h * w)\n",
    "\n",
    "    con_matrix = confusion_matrix(target, pred,labels = np.arange(0,n_class,1))\n",
    "    con_matrix[0][0] = 0\n",
    "    for i in range(0, n_class):\n",
    "        for j in range(0, n_class):\n",
    "            if i == j:\n",
    "                TP[i] += con_matrix[i][j]\n",
    "            if i != j:\n",
    "                FP[j] += con_matrix[i][j]\n",
    "                FN[i] += con_matrix[i][j]\n",
    "    return TP, FN, FP\n",
    "                \n",
    "def pixel_acc(pred, target):\n",
    "    correct = (pred == target).sum()\n",
    "    total   = (target == target).sum()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model_name):\n",
    "    \n",
    "    # load pretrain models\n",
    "              \n",
    "    vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n",
    "    fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)\n",
    "    fcn_model = nn.DataParallel(fcn_model)      \n",
    "    \n",
    "    state_dict = torch.load(os.path.join(model_dir, model_name), map_location='cpu')\n",
    "    fcn_model.load_state_dict(state_dict)\n",
    "    \n",
    "    batch = dataiter.next()\n",
    "    if use_gpu:\n",
    "        inputs = Variable(batch['X'].cuda())\n",
    "    else:\n",
    "        inputs = Variable(batch['X'])\n",
    "    img    = batch['origin'] \n",
    "    label  = batch['l']\n",
    "    \n",
    "    inputs = Variable(batch['X'])\n",
    "    output = fcn_model(inputs)\n",
    "    output = output.data.cpu().numpy()\n",
    "\n",
    "    N, _, h, w = output.shape\n",
    "    pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis = 1).reshape(N, h, w)\n",
    "\n",
    "    # show images\n",
    "    plt.figure(figsize = (10, 12))\n",
    "    img = img.numpy()\n",
    "    for i in range(N):\n",
    "        img[i] = cv2.cvtColor(img[i], cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(N, 3, i*3 + 1)\n",
    "        plt.title(\"origin_img\")\n",
    "        plt.imshow(img[i])\n",
    "        #print(np.unique(_img[i]))\n",
    "\n",
    "        plt.subplot(N, 3, i*3 + 2)\n",
    "        plt.title(\"label_img\")\n",
    "        plt.imshow(label[i],cmap = \"nipy_spectral\",vmin = 0, vmax = n_class - 1)\n",
    "\n",
    "        plt.subplot(N, 3, i*3 + 3)\n",
    "        plt.title(\"prediction\")\n",
    "        plt.imshow(pred[i],cmap = \"nipy_spectral\",vmin = 0, vmax = n_class - 1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_url = \"https://drive.google.com/u/1/uc?id=1IQI4Rvr6fYvvRuWPVKc23c4fioB1USKa&export=download\"\n",
    "models_name = \"FCNs_subt_model_batch8_epoch200_RMSprop_lr0.0001.pkl\"\n",
    "if not os.path.isfile(models_name):\n",
    "    gdown.download(models_url, output=\"models/subt_model/\" + models_name, quiet=False)\n",
    "\n",
    "print(\"Finished downloading models.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(\"FCNs_subt_model_batch8_epoch200_RMSprop_lr0.0001.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to use the model you just trained to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd models/subt_model\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpack = 1, survivor = 2, vent = 3, phone = 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
